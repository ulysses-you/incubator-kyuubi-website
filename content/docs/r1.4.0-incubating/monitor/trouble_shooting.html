

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>3. Trouble Shooting &mdash; Kyuubi 1.4.0-incubating documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="SQL References" href="../sql/index.html" />
    <link rel="prev" title="2. Monitoring Kyuubi - Server Metrics" href="metrics.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Kyuubi
          

          
            
            <img src="../_static/kyuubi_logo_gray.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Usage Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/index.html">Deploying Kyuubi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security/index.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../client/index.html">Client Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../integrations/index.html">Integrations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Monitoring</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="logging.html">1. Monitoring Kyuubi - Logging System</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">2. Monitoring Kyuubi - Server Metrics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3. Trouble Shooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-issues">3.1. Common Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#java-lang-unsupportedclassversionerror-unsupported-major-minor-version-52-0">3.1.1. java.lang.UnsupportedClassVersionError .. Unsupported major.minor version 52.0</a></li>
<li class="toctree-l4"><a class="reference internal" href="#org-apache-spark-sparkexception-when-running-with-master-yarn-either-hadoop-conf-dir-or-yarn-conf-dir-must-be-set-in-the-environment">3.1.2. org.apache.spark.SparkException: When running with master ‘yarn’ either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#javax-security-sasl-saslexception-gss-initiate-failed-caused-by-gssexception-no-valid-credentials-provided-mechanism-level-failed-to-find-any-kerberos-tgt">3.1.3. javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)];</a></li>
<li class="toctree-l4"><a class="reference internal" href="#org-apache-hadoop-security-accesscontrolexception-permission-denied-user-hzyanqin-access-write-inode-user-hdfs-hdfs-drwxr-xr-x">3.1.4. org.apache.hadoop.security.AccessControlException: Permission denied: user=hzyanqin, access=WRITE, inode=”/user”:hdfs:hdfs:drwxr-xr-x</a></li>
<li class="toctree-l4"><a class="reference internal" href="#org-apache-thrift-tapplicationexception-invalid-method-name-get-table-req">3.1.5. org.apache.thrift.TApplicationException: Invalid method name: ‘get_table_req’</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-server2-thrift-max-worker-threads">3.1.6. hive.server2.thrift.max.worker.threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-create-function-using-jar">3.1.7. Failed to create function using jar</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-start-spark-3-1-with-error-msg-cannot-modify-the-value-of-a-spark-config">3.1.8. Failed to start Spark 3.1 with error msg ‘Cannot modify the value of a Spark config’</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sql/index.html">SQL References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/index.html">Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">Kyuubi Insider</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview/index.html">Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../develop_tools/index.html">Develop Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/index.html">Community</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix/index.html">Appendixes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Kyuubi</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Monitoring</a> &raquo;</li>
        
      <li><span class="section-number">3. </span>Trouble Shooting</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/monitor/trouble_shooting.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!--
 - Licensed to the Apache Software Foundation (ASF) under one or more
 - contributor license agreements.  See the NOTICE file distributed with
 - this work for additional information regarding copyright ownership.
 - The ASF licenses this file to You under the Apache License, Version 2.0
 - (the "License"); you may not use this file except in compliance with
 - the License.  You may obtain a copy of the License at
 -
 -   http://www.apache.org/licenses/LICENSE-2.0
 -
 - Unless required by applicable law or agreed to in writing, software
 - distributed under the License is distributed on an "AS IS" BASIS,
 - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 - See the License for the specific language governing permissions and
 - limitations under the License.
 --><div align=center><p><img alt="../_images/kyuubi_logo.png" src="../_images/kyuubi_logo.png" /></p>
</div><div class="section" id="trouble-shooting">
<h1><span class="section-number">3. </span>Trouble Shooting<a class="headerlink" href="#trouble-shooting" title="Permalink to this headline">¶</a></h1>
<div class="section" id="common-issues">
<h2><span class="section-number">3.1. </span>Common Issues<a class="headerlink" href="#common-issues" title="Permalink to this headline">¶</a></h2>
<div class="section" id="java-lang-unsupportedclassversionerror-unsupported-major-minor-version-52-0">
<h3><span class="section-number">3.1.1. </span>java.lang.UnsupportedClassVersionError .. Unsupported major.minor version 52.0<a class="headerlink" href="#java-lang-unsupportedclassversionerror-unsupported-major-minor-version-52-0" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/apache/kyuubi/server/KyuubiServer : Unsupported major.minor version 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:803)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:442)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:64)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:354)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:347)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:312)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
</pre></div>
</div>
<p>Firstly, you should check the version of Java JRE used to run Kyuubi is actually matched with the version of Java compiler used to build Kyuubi.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ java -version
java version <span class="s2">&quot;1.7.0_171&quot;</span>
OpenJDK Runtime Environment <span class="o">(</span>rhel-2.6.13.2.el7-x86_64 u171-b01<span class="o">)</span>
OpenJDK <span class="m">64</span>-Bit Server VM <span class="o">(</span>build <span class="m">24</span>.171-b01, mixed mode<span class="o">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ cat RELEASE
Kyuubi <span class="m">1</span>.0.0-SNAPSHOT <span class="o">(</span>git revision 39e5da5<span class="o">)</span> built <span class="k">for</span>
Java <span class="m">1</span>.8.0_251
Scala <span class="m">2</span>.12
Spark <span class="m">3</span>.0.1
Hadoop <span class="m">2</span>.7.4
Hive <span class="m">2</span>.3.7
Build flags:
</pre></div>
</div>
<p>To fix this problem you should export <code class="docutils literal notranslate"><span class="pre">JAVA_HOME</span></code> with a compatible one in <code class="docutils literal notranslate"><span class="pre">conf/kyuubi-env.sh</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span> <span class="s2">&quot;export JAVA_HOME=/path/to/jdk1.8.0_251&quot;</span> &gt;&gt; conf/kyuubi-env.sh
</pre></div>
</div>
</div>
<div class="section" id="org-apache-spark-sparkexception-when-running-with-master-yarn-either-hadoop-conf-dir-or-yarn-conf-dir-must-be-set-in-the-environment">
<h3><span class="section-number">3.1.2. </span>org.apache.spark.SparkException: When running with master ‘yarn’ either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment<a class="headerlink" href="#org-apache-spark-sparkexception-when-running-with-master-yarn-either-hadoop-conf-dir-or-yarn-conf-dir-must-be-set-in-the-environment" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Exception in thread &quot;main&quot; org.apache.spark.SparkException: When running with master &#39;yarn&#39; either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
	at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:630)
	at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:270)
	at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:233)
	at org.apache.spark.deploy.SparkSubmitArguments.&lt;init&gt;(SparkSubmitArguments.scala:119)
	at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.&lt;init&gt;(SparkSubmit.scala:990)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:990)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</pre></div>
</div>
<p>When Kyuubi gets the <code class="docutils literal notranslate"><span class="pre">spark.master=yarn</span></code>, <code class="docutils literal notranslate"><span class="pre">HADOOP_CONF_DIR</span></code> should also be exported in <code class="docutils literal notranslate"><span class="pre">$KYUUBI_HOME/conf/kyuubi-env.sh</span></code>.</p>
<p>To fix this problem you should export <code class="docutils literal notranslate"><span class="pre">HADOOP_CONF_DIR</span></code> to the folder that contains the hadoop client settings in <code class="docutils literal notranslate"><span class="pre">conf/kyuubi-env.sh</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span> <span class="s2">&quot;export HADOOP_CONF_DIR=/path/to/hadoop/conf&quot;</span> &gt;&gt; conf/kyuubi-env.sh
</pre></div>
</div>
</div>
<div class="section" id="javax-security-sasl-saslexception-gss-initiate-failed-caused-by-gssexception-no-valid-credentials-provided-mechanism-level-failed-to-find-any-kerberos-tgt">
<h3><span class="section-number">3.1.3. </span>javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)];<a class="headerlink" href="#javax-security-sasl-saslexception-gss-initiate-failed-caused-by-gssexception-no-valid-credentials-provided-mechanism-level-failed-to-find-any-kerberos-tgt" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="org-apache-hadoop-security-accesscontrolexception-permission-denied-user-hzyanqin-access-write-inode-user-hdfs-hdfs-drwxr-xr-x">
<h3><span class="section-number">3.1.4. </span>org.apache.hadoop.security.AccessControlException: Permission denied: user=hzyanqin, access=WRITE, inode=”/user”:hdfs:hdfs:drwxr-xr-x<a class="headerlink" href="#org-apache-hadoop-security-accesscontrolexception-permission-denied-user-hzyanqin-access-write-inode-user-hdfs-hdfs-drwxr-xr-x" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>org.apache.hadoop.security.AccessControlException: Permission denied: user=hzyanqin, access=WRITE, inode=&quot;/user&quot;:hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:350)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:251)
	at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:306)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1767)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1710)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3062)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1156)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3007)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2975)
	at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1047)
	at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1043)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1061)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1036)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:600)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:441)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:876)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:196)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:60)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:201)
	at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:555)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2574)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:934)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:928)
	at org.apache.kyuubi.engine.spark.SparkSQLEngine$.createSpark(SparkSQLEngine.scala:72)
	at org.apache.kyuubi.engine.spark.SparkSQLEngine$.main(SparkSQLEngine.scala:101)
	at org.apache.kyuubi.engine.spark.SparkSQLEngine.main(SparkSQLEngine.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</pre></div>
</div>
<p>The user do not have permission to create to Hadoop home dir, which is <code class="docutils literal notranslate"><span class="pre">/user/hzyanqin</span></code> in the case above.</p>
<p>To fix this problem you need to create this directory first and grant ACL permission for <code class="docutils literal notranslate"><span class="pre">hzyanqin</span></code>.</p>
</div>
<div class="section" id="org-apache-thrift-tapplicationexception-invalid-method-name-get-table-req">
<h3><span class="section-number">3.1.5. </span>org.apache.thrift.TApplicationException: Invalid method name: ‘get_table_req’<a class="headerlink" href="#org-apache-thrift-tapplicationexception-invalid-method-name-get-table-req" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Caused by: org.apache.thrift.TApplicationException: Invalid method name: &#39;get_table_req&#39;
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table_req(ThriftHiveMetastore.java:1567)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table_req(ThriftHiveMetastore.java:1554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1350)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy37.getTable(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2336)
	at com.sun.proxy.$Proxy37.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1274)
	... 93 more
</pre></div>
</div>
<p>This error means that you are using incompatible version of Hive metastore client to connect the Hive metastore server.</p>
<p>To fix this problem you could use a compatible version of Hive client by configuring
<code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.jars</span></code> and <code class="docutils literal notranslate"><span class="pre">spark.sql.hive.metastore.version</span></code> at Spark side.</p>
</div>
<div class="section" id="hive-server2-thrift-max-worker-threads">
<h3><span class="section-number">3.1.6. </span>hive.server2.thrift.max.worker.threads<a class="headerlink" href="#hive-server2-thrift-max-worker-threads" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Unexpected</span> <span class="n">end</span> <span class="n">of</span> <span class="n">file</span> <span class="n">when</span> <span class="n">reading</span> <span class="kn">from</span> <span class="nn">HS2</span> <span class="n">server</span><span class="o">.</span> <span class="n">The</span> <span class="n">root</span> <span class="n">cause</span> <span class="n">might</span> <span class="n">be</span> <span class="n">too</span> <span class="n">many</span> <span class="n">concurrent</span> <span class="n">connections</span><span class="o">.</span> <span class="n">Please</span> <span class="n">ask</span> <span class="n">the</span> <span class="n">administrator</span> <span class="n">to</span> <span class="n">check</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">active</span> <span class="n">connections</span><span class="p">,</span> <span class="ow">and</span> <span class="n">adjust</span> <span class="n">hive</span><span class="o">.</span><span class="n">server2</span><span class="o">.</span><span class="n">thrift</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">threads</span> <span class="k">if</span> <span class="n">applicable</span><span class="o">.</span>
<span class="n">Error</span><span class="p">:</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">thrift</span><span class="o">.</span><span class="n">transport</span><span class="o">.</span><span class="n">TTransportException</span> <span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="mi">08</span><span class="n">S01</span><span class="p">,</span><span class="n">code</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>In Kyuubi, we should increase <code class="docutils literal notranslate"><span class="pre">kyuubi.frontend.min.worker.threads</span></code> instead of <code class="docutils literal notranslate"><span class="pre">hive.server2.thrift.max.worker.threads</span></code></p>
</div>
<div class="section" id="failed-to-create-function-using-jar">
<h3><span class="section-number">3.1.7. </span>Failed to create function using jar<a class="headerlink" href="#failed-to-create-function-using-jar" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TEMPORARY</span> <span class="pre">FUNCTION</span> <span class="pre">TEST</span> <span class="pre">AS</span> <span class="pre">'com.netease.UDFTest'</span> <span class="pre">using</span> <span class="pre">jar</span> <span class="pre">'hdfs:///tmp/udf.jar'</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Error operating EXECUTE_STATEMENT: org.apache.spark.sql.AnalysisException: Can not load class &#39;com.netease.UDFTest&#39; when registering the function &#39;test&#39;, please make sure it is on the classpath;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$registerFunction$1(SessionCatalog.scala:1336)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.registerFunction(SessionCatalog.scala:1333)
	at org.apache.spark.sql.execution.command.CreateFunctionCommand.run(functions.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)
	at org.apache.kyuubi.engine.spark.operation.ExecuteStatement.org$apache$kyuubi$engine$spark$operation$ExecuteStatement$$executeStatement(ExecuteStatement.scala:64)
	at org.apache.kyuubi.engine.spark.operation.ExecuteStatement$$anon$1.run(ExecuteStatement.scala:80)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
</pre></div>
</div>
<p>If you get this exception when creating a function, you can check your JDK version.
You should update JDK to JDK1.8.0_121 and later, since JDK1.8.0_121 fix a security issue <a class="reference external" href="https://www.oracle.com/java/technologies/javase/8u121-relnotes.html">Additional access restrictions for URLClassLoader.newInstance</a>.</p>
</div>
<div class="section" id="failed-to-start-spark-3-1-with-error-msg-cannot-modify-the-value-of-a-spark-config">
<h3><span class="section-number">3.1.8. </span>Failed to start Spark 3.1 with error msg ‘Cannot modify the value of a Spark config’<a class="headerlink" href="#failed-to-start-spark-3-1-with-error-msg-cannot-modify-the-value-of-a-spark-config" title="Permalink to this headline">¶</a></h3>
<p>Here is the error message</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Caused by: org.apache.spark.sql.AnalysisException: Cannot modify the value of a Spark config: spark.yarn.queue
	at org.apache.spark.sql.RuntimeConfig.requireNonStaticConf(RuntimeConfig.scala:156)
	at org.apache.spark.sql.RuntimeConfig.set(RuntimeConfig.scala:40)
	at org.apache.kyuubi.engine.spark.session.SparkSQLSessionManager.$anonfun$openSession$2(SparkSQLSessionManager.scala:68)
	at org.apache.kyuubi.engine.spark.session.SparkSQLSessionManager.$anonfun$openSession$2$adapted(SparkSQLSessionManager.scala:56)
	at scala.collection.immutable.Map$Map4.foreach(Map.scala:236)
	at org.apache.kyuubi.engine.spark.session.SparkSQLSessionManager.openSession(SparkSQLSessionManager.scala:56)
	... 12 more
</pre></div>
</div>
<p>This is because Spark-3.1 will check the config which you set and throw exception if the config is static or used in other module (e.g. yarn/core).</p>
<p>You can add a config <code class="docutils literal notranslate"><span class="pre">spark.sql.legacy.setCommandRejectsSparkCoreConfs=false</span></code> in <code class="docutils literal notranslate"><span class="pre">spark-defaults.conf</span></code> to disable this behavior.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../sql/index.html" class="btn btn-neutral float-right" title="SQL References" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="metrics.html" class="btn btn-neutral float-left" title="2. Monitoring Kyuubi - Server Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the &#34;License&#34;); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>